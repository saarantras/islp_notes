{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55f0ae8-13a3-4025-b25b-6b01f61f0331",
   "metadata": {},
   "source": [
    "## Estimating a relationship \" $f$ \"\n",
    "\n",
    "\n",
    "- $X=(X_1 + X_2 + X_3 ... X_p)$ : the \"input variables\" or \"predictors\". \n",
    "- $Y$ : the \"output variable\" or \"response variable\". \n",
    "- \"noise\" : $\\epsilon$\n",
    "\n",
    "We hypothesize that, in reality, there exists some relationship, $Y=f(X)+\\epsilon$. The true equation for $f$ is almost never known.\n",
    "\n",
    "### Why do we want to estimate $f$?\n",
    "\n",
    "This function, $f(X)$ is of much interest. We may wish to obtain a reasonable estimate for it (denoted $\\hat{f}(X)$ : said \"f-hat of x\" : hat usually means \"estimate of\"), for two reasons. \n",
    "\n",
    "1. Prediction.\n",
    "\n",
    "- It may be easy to measure $X$ and hard to measure $Y$. For example, it might be \"hard\" to measure the amount of DNA in a sample (what are you going to do, count the molecules?) but \"easy\" to measure how much light is absorbed by the sample. So we want to determine the an estimate for the relationship between light absorbed ($X$) and DNA in the sample ($Y$), so we can use absorbance to estimate DNA concentration with $Y\\approx\\hat{Y}=\\hat{f}(X)$ (like on the nanodrop). \n",
    "\n",
    " - Once we have this estimate $\\hat{f}$, then, to obtain an estimate of $Y$ (denoted $\\hat{Y}$) we may first observe $X$, then compute $\\hat{Y}=\\hat{f}(X)$.\n",
    "- The accuracy of $\\hat{Y}$ as an estimate of $Y$ will depend on two sources of error.\n",
    "    - reducible error : The difference between $f$ and $\\hat{f}$. Can be reduced by better estimates of $f$\n",
    "    - irreducible error : Even if we find the exact function such that $f=\\hat{f}$, we still have some error, as \n",
    "        - $\\hat{Y}=\\hat{f}(X)=f(X)\\neq f(X)+\\epsilon=Y$\n",
    "    - irreducible error comes from $\\epsilon$\n",
    "\n",
    "2.Inference\n",
    "\n",
    "- The relationship between $X$ and $Y$ may itself be of interest. For example:\n",
    "    - Which predictors in $X$ are the most important? (\"Which genetic locus is most associated with risk of xyz disease?\")\n",
    "    - What is the relationship of particular predictors with $Y$? Is the relationship positive? Negative? Of what magnitude?\n",
    "    - (\"Is an A allele associated with more or less xyz disease? How much more or less?\")\n",
    "    - Is the relationship between the predictors and $Y$ linear? Or something else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b99061-5ab1-47d5-83d8-6f32c5456b54",
   "metadata": {},
   "source": [
    "### There are two ways we can estimate $f$.\n",
    "\n",
    "Both use training some set of $n$ \"training data\" points.\n",
    "\n",
    "$x_{ij}$ represents the $j$th predictor of the $i$th data point. In total, we have $p$ predictors and $n$ data points.\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1i} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2i} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{p1} & x_{p2} & \\cdots & x_{pi}\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "Then the training data consist of \n",
    "$\\{ (x_1,y_1),(x_2,y_2), \\cdots , (x_n,y_n) \\}$ \n",
    "\n",
    "Where \n",
    "$\n",
    "x_i=\\begin{bmatrix}\n",
    "x_{i1} \\\\\n",
    "x_{i2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{ip}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "(That is: it's a column of the above matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2a3ab-1071-4f99-abf4-ed59d29ee35a",
   "metadata": {},
   "source": [
    "The two ways:\n",
    "\n",
    "1. Parametric estimation\n",
    "\n",
    "In parametric estimation, we guess what the form of the equation $f$ is, then use data to \"fill in the blanks\". For example, we might guess the following form:\n",
    "\n",
    "$f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p$\n",
    "\n",
    "A concrete example of what a model like this might look like is:\n",
    "\n",
    "$income=\\beta_0+\\beta_1\\cdot education+\\beta_1\\cdot seniority$\n",
    "\n",
    "For preducting income based on an employees education \n",
    "\n",
    "In this case, our parametric assumption is a \"linear relationship\" between the predictors and output. We would then use our training data to \"fill in\" all the $\\beta$ values (the parameters). There are many ways to do this. For linear models, one of the most popular approches is called \"least squares\". \n",
    "\n",
    "You can also see how the $\\beta$ values have comprehensible meanings. If $\\beta_1>0$, then more education means more pay. If $\\beta_1=0$ then the amount of education doesn't matter for pay. If $\\beta_1<0$, then the more educated you are the less you are paid. In the cases where $\\beta_1\\neq0$ the magnitude of $\\beta_1$ determines *how much* education impacts pay.\n",
    "\n",
    "2. Non-parametric estimation\n",
    "\n",
    "Pro: We don't have to guess at the function, so we can't guess wrong.\n",
    "Con: We need way more data.\n",
    "\n",
    "You still need to set certain values, like \"smoothness\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77739ff-7034-444c-8ed0-1f4c535466b7",
   "metadata": {},
   "source": [
    "## Flexibility-interpretability tradeoff. \n",
    "\n",
    "Some methods of estimating $f$ are very \"interpretable\". For example, as shown above, assuming a linear relationship gives us a bunch of $\\beta$ values whose meanings are pretty clear ($\\beta_1$ means : is more education good or bad for making more money?). However, this approach is not very flexible : it only works well when the relationships are actually more-or-less linear : otherwise we might say the model is \"misspecified\", and we will get bad predictions and bad inferences. Other methods of estimating $f$ are more flexible (for example, will work in more cases than just if $f$ is nonlinear) but less interpretable. The most dramatic example is probably \"deep learning\" (AKA neural networks). They can do some really nifty things, but no one really knows what any of the values in their guts mean. \n",
    "\n",
    "## Supervised vs unsupervised learning.\n",
    "\n",
    "Who is doing the \"supervising\"? The output variable $Y$. Consequently, when we do have real measurements of $Y$ in our data (actual salary figures) then the learning is \"supervised\". All the examples above are \"supervised\". When we don't have a specific thing we wish to predict and have data for, then the learning is \"unsupervised\". An example of unsupervised learning would be \"clustering\", which takes a bunch of data points and assigns them into some some number of groups based on their similarity to one another. For example, if you fed a clustering algorithm a big database where each data-point was a person, and each predictor was the star rating they assigned a movie on netflix, the algorithm might realize that people who rated \"inception\" five stars also rated \"tenet\" and \"dunkirk\" highly, and might assign them to a cluster. (which in turn represents a \"hidden\" variable : \"likes chrisopher nolan movies\").\n",
    "\n",
    "## Regression vs classification\n",
    "\n",
    "When we are performing supervised learning, our approach can be said to be either \"regression\" or \"classification\". The only difference is what $Y$ is : is it quantatative (a number)? Then it's regression. Is it categorical (one of a set of options, like \"Acute Myelogenous Leukemia\", \"Acute Lymphoblastic Leukemia\", or \"No Leukemia\") ? Then it's \"classification\". Note that whether the *predictor* is quantattive or categorical is generally considered unimportant. It's not always obvious which variables are quantative vs categorical. Sometimes categories have numerical codes, and sometimes individual numbers will have specific meanings. A rule of thumb I like to use is : \"does the order matter\"? $1$ apple is closer to $2$ apples than $10$ apples, (\"apples\" is quantative) but \"Leukemia\" isn't really closer to or farther from \"Ulcer\" than \"Hemorrhage\" (\"diagnosis\" is categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42593ea-c7c2-4bbd-8a9d-ac308944e2cd",
   "metadata": {},
   "source": [
    "## Measuring model accuracy\n",
    "\n",
    "How close is $\\hat{f}$ to $f$? Which of the many, many methods for getting $\\hat{f}$ is best for your particular problem? In general, we'd like to put a number to these questions : to quantify the error. In this way, we have a number that tells us how good a job we are doing, and if what we just did was good (made the error go down) or bad (made the error go up).\n",
    "\n",
    "When doing regressions, a popular choice to measure error is \"Mean Squared Error\" or MSE\n",
    "\n",
    "$MSE=\\frac{1}{n}\\sum^n_{i=1}(y_i-\\hat{f}(x_i))^2$\n",
    "\n",
    "When doing classification a popular choice is the \"error rate\":\n",
    "\n",
    "$ER=\\frac{1}{n}\\sum_{i=1}^nI(y_i\\neq\\hat{y}_i)$\n",
    "\n",
    "Where $I$ considers the enclosed expression and returns $1$ if true and $0$ if false. Thus, the \"error rate\" is \"fraction incorrect classifications\".\n",
    "\n",
    "Note that this and all other measures of error really have two versions : the \"training\" and \"test\" versions. The \"training MSE\" would be the MSE when our $\\hat{f}$ is used on data *that was used to create $\\hat{f}$ in the first place*. The \"test MSE\" is the MSE when our $\\hat{f}$ is used on data that it's never seen before. \n",
    "\n",
    "## Bias-variance tradeoff\n",
    "\n",
    "Note that different choices made in our approach can have different effects on these two errors. It's common that allowing your approach to be more flexible and more closely match the training data will always lead to lower training error, but will only lead to lower test error up to a point, after which test error will go back up again. This \"point\" is the point at which our model starts contorting itself not just to fit $f(X)$ but also $\\epsilon$. In general, we expect the major trends in the data to be determined by $f(X)$, and minor perturbations to be caused by $\\epsilon$ (if this is not the case, we are doomed before we begin : nothing can be learned when the signal is swamped by noise). As we increase the flexibility of our model, we expect it to change from fitting the major trends to the minor trends. \n",
    "\n",
    "For some datapoint indexed $0$\n",
    "\n",
    "$E[(y_0-\\hat{f})^2]=Variance(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2+Variance(\\epsilon)$\n",
    "\n",
    "So the expected MSE at a particular data point is how variable teh estimate is at that point, plus how biased the estimate the estimate is at that point squared, plus how variable the error is. \n",
    "\n",
    "Variance is : how much $\\hat{f}$ would change if we created it using different training data. High model flexibility lends itself to high variance : as the model can contort itself to perfectly match the training data. \n",
    "\n",
    "Bias is : how different our preconcieved notions about the form of the model are from reality. High model flexibility lends itself to low bias : as the model can contort itself to match the data and so bad assumptions are not as big a deal. \n",
    "\n",
    "Consider a model as we increase flexability. At low flexibility, the variability of $\\hat{f}$ is low, but the bias is high (consistent : and consistently reports the wrong value). At high flexibility, the varibility of $\\hat{f}$ is high, but the bias is low (very inconsistent, but matches the training data perfectly). To pull signal from noise, we want a happy intermediate : reports basically the right value, and is not perturbed by variable noise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be674eb6-060a-4742-929d-e747c0285c35",
   "metadata": {},
   "source": [
    "## The bayes classifier\n",
    "\n",
    "A \"classifier\" is a function which performs classification. It can be shown that the error rate (defined above) of a classifier is minimized when it assigns every data point to its most likely class. That is, it maximizes $Pr(Y=j|X=x_0)$. This is called the \"bayes classifier\". \n",
    "\n",
    "Its error rate is $1-E[max_j Pr(Y=j|X)]$ where expectation is taken over all values of X. (So basically : the probability of all classifications other than the most likley/correct one).\n",
    "\n",
    "This \"Bayes Error Rate\" is analogous to irreducible error. \n",
    "\n",
    "Of course, $Pr(Y|X)$ is unknown for all real examples. Therefore, many approaches use an estimate of that probability distribution. One popular method for doing this is the \"K-nearest-neighbors\"\n",
    "\n",
    "### K-nearest-neighbors\n",
    "\n",
    "This approach takes some training data with known classifications and, for some unknown point, will approximate $Pr(Y=j|X=x_0)$ from the known data. \n",
    "\n",
    "For some data point $x_0$ we pick out the the $K$ nearest neighbors (from the known data), their indicies designated $\\mathcal{N}$\n",
    "\n",
    "$Pr(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}_0 }I(y_i=j)$\n",
    "\n",
    "\"The probability that $Y$ is should be classified $j$, given that $X$ is $x_0$ is equal to the fraction of neighbors of this data point who are classified as $j$\".\n",
    "\n",
    "Then the $j$ that leads to the highest $Pr(Y=j|X=x_0)$ is assigned to $Y$. \n",
    "\n",
    "The flexibility of this approach varies with $1/K$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
