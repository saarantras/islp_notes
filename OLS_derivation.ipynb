{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd58576-593c-4e33-b0bd-b3553fd293cc",
   "metadata": {},
   "source": [
    "UNFINISHED\n",
    "\n",
    "Supplemental material for chapter 3 of ISLP. Adapted (stolen) from https://www.statlect.com/fundamentals-of-statistics/linear-regression. \n",
    "\n",
    "Our linear regression takes the form\n",
    "\n",
    "$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+ \\cdots \\beta_pX_p+\\epsilon$\n",
    "\n",
    "Or, in matrix notation,\n",
    "\n",
    "\n",
    "$Y=X\\beta+\\epsilon$\n",
    "\n",
    "expanded as\n",
    "\n",
    "In the center $X$ matrix : In total, we have $p$ predictors and $n$ data points.\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "    1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "    \\vdots &\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\beta_0 \\\\\n",
    "    \\beta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\beta_p \\\\\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "    \\epsilon_0 \\\\\n",
    "    \\epsilon_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\epsilon_n \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "We compute our estimates of the betas as (here in matricies and vectors again). \n",
    "\n",
    "$\n",
    "\\hat{\\beta}=\\underset{b}{\\operatorname{arg min}}\\Sigma_{i=1}^n(y_i-x_ib)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219ebd9-e222-409d-834d-7720ef48190b",
   "metadata": {},
   "source": [
    "Which makes intuitive sense. We can state this \"ordinary least squares\" estimator as \"across all data points, the set of betas that result in the smallest squared difference between our presicted and observed response\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff67ae-7778-4fc5-8a9c-babd25f19c25",
   "metadata": {},
   "source": [
    "**This can be re-written as** $\\hat{\\beta}=(X^TX)^{-1}X^Ty$, a formula for computing OLS estimates!\n",
    "\n",
    "To convince ourselves of this, here a proof:\n",
    "\n",
    "First, we take the OLS sum (but not argmin-part) expression above, and rewrite it as follows:\n",
    "\n",
    "$\\Sigma_{i=1}^n(y_i-x_ib)^2=(y-Xb)^T(y-Xb)$\n",
    "\n",
    "To convince you we can do this re-write, here is an expansion:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_n \\\\\n",
    "    \\end{bmatrix}-\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "        1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "        \\vdots &\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\beta_0 \\\\\n",
    "        \\beta_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_p \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_n \\\\\n",
    "    \\end{bmatrix}-\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "        1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "        \\vdots &\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\beta_0 \\\\\n",
    "        \\beta_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_p \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770f858-19ae-4518-8728-229ad618bcde",
   "metadata": {},
   "source": [
    "Now we want to find the $b$ that minimizes $(y-Xb)^T(y-Xb)$\n",
    "\n",
    "In one-dimensional optimization problems, we would identify locations where the deriviative was equal to zero : as these would be minima and maxima. For multi-dimensional optimization problems, we take the multi-dimensional equivalent of the deriviative, the gradient, and set it to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f56949-ed53-4b3f-ab03-db01693804a3",
   "metadata": {},
   "source": [
    "$\\nabla_b (y-Xb)^T(y-Xb) =0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
