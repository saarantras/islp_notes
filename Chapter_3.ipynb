{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aeb286-27c9-456d-868a-bde09ab2ce63",
   "metadata": {},
   "source": [
    "# Simple linear regression\n",
    "\n",
    "[Some material taken from statlect](https://www.statlect.com/glossary/mean-squared-error)\n",
    "\n",
    "A **simple** linear model takes a form like $Y\\approx \\beta_0 + \\beta_1 X$ where we use $X$ to predict $Y$. Here, $\\approx$ represents \"is approximately modeled as\". The above expression may also be phrased \"regressing $Y$ onto $X$\" or \"regressing $Y$ on $X$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676a99f-cde1-4c6c-ba9a-6a9253ac1c87",
   "metadata": {},
   "source": [
    "## Estimating the coefficents \n",
    "\n",
    "The betas are the \"coefficients\" or \"parameters\". We estimate them as $\\hat{\\beta_0}, \\hat{\\beta_1}$\n",
    "\n",
    "We want to find the line that matches the data most similarly. We can do this in different ways. One common approaches is the \"least squares\" criterion. Let's do that.\n",
    "\n",
    "Residual Sum of Squares$=RSS=(y_1-\\hat{y_1})^2+(y_2-\\hat{y_2})^2+ \\cdots + (y_n-\\hat{y_n})^2$\n",
    "\n",
    "Essentially: the sum of the squares of the differences between the predictions and real values. Each term is a \"residual\", and is summarized $e_i=(y_n-\\hat{y_n})$. Therefore RSS can be written\n",
    "\n",
    "$RSS=e_1^2+e_2^2 \\cdots e_n^2$\n",
    "\n",
    "Going the opposite direction, and expanding terms instead of compressing, them: $\\hat{y_i}=\\beta_0+\\beta_1x_i$, Therefore:\n",
    "\n",
    "$RSS=(y_1-(\\beta_0+\\beta_1x_1))^2+(y_2-(\\beta_0+\\beta_1x_2))^2 \\cdots (y_i-(\\beta_0+\\beta_1x_i))^2$\n",
    "\n",
    "**Aside:**\n",
    "- \"Residual\" I think is a term specifically associated with linear regression. \"Mean Squared Error\" (MSE) on the other hand, is more general. For some estimator $\\hat{Z}$ of parameter $Z$, we might say $MSE=E[||\\hat{Z}-Z||^2]$. In our case, $MSE=\\frac{1}{N}RSS$\n",
    "\n",
    "\n",
    "RSS is minimized when the coefficients are:\n",
    "\n",
    "$\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum^n_{i=1}(x_i-\\bar{x})^2}$\n",
    "\n",
    "$\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta \\bar{x}}$\n",
    "\n",
    "($\\bar{x}$ and $\\bar{y}$ are sample means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4793d-9546-4ba5-8c0f-0d45dc740023",
   "metadata": {},
   "source": [
    "## Assessing the accuracy of the model\n",
    "\n",
    "The \"population regression line\" is the true regression line for the real populaton, and is therefore unknown. Our least-squares regression line is an estimate of this line, based on sample data. This is analogous to estimating the population mean from a sample mean. \n",
    "\n",
    "### Bias of the model\n",
    "\n",
    "The bias of an estimator is the expected difference between the estimator and the true value of the parameter.\n",
    "- For some estimator $\\hat{Z}$ of $Z$, $Bias[\\hat{Z}]=E[\\hat{Z}]-Z$\n",
    "- Essentially, it's how much our estimator systemically deviates from the true parameter.\n",
    "\n",
    "The least-squares approach to estimate $\\beta_0$ and $\\beta_1$ outlined above is **unbiased**. \n",
    "\n",
    "### Variance of the model\n",
    "\n",
    "That said, any given realization of the estimators $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ are going to be different from the true values. It would be nice to know something about the distribution of the estimates : about how far are they going to be from the real values. To this end, we can compute an estimate of the standard error of $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ as follows:\n",
    "\n",
    "$\\widehat{SE(\\hat{\\beta_0})}^2=\\sigma^2[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}]$\n",
    "\n",
    "$\\widehat{SE(\\hat\\beta_1)}^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n",
    "\n",
    "We can estimate $\\sigma$ from the data as $RSE=\\sqrt{RSS/(n-2)}$\n",
    "\n",
    "Note that this assumes that the errors $\\epsilon$ for each observation have the same variance and are uncorrelated. \n",
    "\n",
    "But I promised you that this $SE$ value relates to the spread of the estimations. And it does! \n",
    "\n",
    "There is a 95% chance that the interval $[\\hat{\\beta_1}-2\\cdot SE(\\beta_1) , \\hat{\\beta_1}+2\\cdot SE(\\beta_1)]$ contains the true value of $\\beta_1$.\n",
    "\n",
    "The same is true for $\\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d7872-239a-44f7-b4a0-a4d5b8194b32",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "\n",
    "We can use the confidence intervals defined above to test hypothesies. Suppose we have:\n",
    "\n",
    "Null hypothesis $H_0$ : There is no relationship between X and Y. : $\\beta_1=0$\n",
    "\n",
    "Alternative hypothesis $H_a$ : There is some relationship between X and Y. : $\\beta_1\\ne0$\n",
    "\n",
    "To examine this we compute the t statistic, $t=\\frac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})}$\n",
    "\n",
    "This figure shows how many standard errors the estimate of $\\beta_1$ is from $0$. The distribution of this t statistic is known in the case that there is really no relationship (it's a t distribution) so we can compute the probability of having such an extreme t-stat by chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6c7a3-d95f-4156-a13f-53e5fe33bad8",
   "metadata": {},
   "source": [
    "### To what extent does the model fit the data?\n",
    "\n",
    "Suppose we have gotten a signifcant p-value from the above, and we reject the null hypothesis that there is no relationship between $X$ and $Y$. We still don't know how good the model is. To establish that, we compute\n",
    "\n",
    "Residual Standard Error $=RSE=\\sqrt{\\frac{1}{n-2}RSS}$\n",
    "\n",
    "Basically the $RSS$ normalized by the total number of predictions & rooted. It's a measure of lack-of-fit of the model to the data : higher=more ill-fitting. It's measured in uints of $Y$. An alternative is the $R^2$ statistic, computed \n",
    "\n",
    "$R^2=1-\\frac{RSS}{TSS}$\n",
    "\n",
    "Where Total Sum of Squares$=TSS=\\sum(y_i-\\bar{y})^2$\n",
    "\n",
    "$TSS$ measures the total amount of variability in the response variable. $RSS$ measures the amount of variability in the response variable not explained post regression. **Therefore $R^2$ is the fraction of total variability in the response variable that is explained by the predictor $X$.** For simple linear regression, this is the same as \"correlation\" squared. However, for more predictors, $R^2$ extends and correlation does not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279614b-08f9-4ca3-a1b8-65d1623e071a",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "For multiple linear regression, we have additional predictors to base our prediction on. This takes the form:\n",
    "\n",
    "$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+ \\cdots \\beta_pX_p+\\epsilon$\n",
    "\n",
    "Each $\\beta_i$ represents the average effect of $X_i$ on $Y$, while holding all other predictors fixed. \n",
    "\n",
    "## Estimating multiple linear regression coefficients. \n",
    "\n",
    "Again, we will never really know any $\\beta_i$, and so we will estimate various $\\hat{\\beta_i}$, composing:\n",
    "\n",
    "$y=\\hat{\\beta_0}+\\hat{\\beta_1}x_1+\\hat{\\beta_2}x_2+ \\cdots \\hat{\\beta_p}x_p+\\epsilon$\n",
    "\n",
    "Sum of squared redisuals can be computed in basically the exact same way:\n",
    "\n",
    "$\\Sigma_{i=1}^n(y_i-\\hat{y_i})^2$\n",
    "\n",
    "However, the method for estimating the varions $\\hat{\\beta_i}$ is more complicated. The book glosses over it, and it's not necessarially important to know (?) But I want to know. \n",
    "\n",
    "Essentially, the multiple linear regression model can be written as : \n",
    "\n",
    "$Y=X\\beta+\\epsilon$\n",
    "\n",
    "But where the variables are vectors and matricies. Consider:\n",
    "\n",
    "\n",
    "\n",
    "This \"$X$ matrix\" i actually called the \"design matrix\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fbd019-75e3-43ee-bb50-45c9988b40fa",
   "metadata": {},
   "source": [
    "See how the dimensions of the center matrix vector multiplication is $(n \\times p)( p \\times 1)=n\\times1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5d46a-3756-4d7a-90ce-59e28160d3a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The OLS estimator for some vector $\\beta$ is\n",
    "\n",
    "$\n",
    "\\hat{\\beta}=\\underset{b}{\\operatorname{arg min}}\\Sigma_{i=1}^n(y_i-x_ib)^2\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dbaf0-26b6-4e69-9773-f2febfed6b15",
   "metadata": {},
   "source": [
    "Or in other words, the OLS estimator for a set of betas is the vector of betas that minimize the squared difference betwen prediction and actual. It can be computed\n",
    "\n",
    "The book doesn't have a formula to find this, or a proof, but I copied one from statlect. See the attached \"OLS_derivation\" file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
